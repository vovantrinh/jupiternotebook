{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SSD MVTec Anomaly Detection Analysis\n",
        "\n",
        "This notebook provides comprehensive analysis of the SSD model trained on MVTec dataset, including:\n",
        "- Dataset statistics\n",
        "- Training configuration and results\n",
        "- Model performance metrics\n",
        "- Qualitative inspection with visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torchvision.models.detection import ssd300_vgg16\n",
        "from torchvision.models.detection.ssd import SSDClassificationHead\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE PATHS\n",
        "BASE_DIR = Path(\"/Users/vantrinh/Work/python/yolo\")\n",
        "DATASET_DIR = BASE_DIR / \"datasets/ssd_mvtec_bottle\"\n",
        "MODEL_DIR = BASE_DIR / \"model_ssd/ssd_mvtec_20251120_215313\"  # UPDATE THIS\n",
        "MODEL_PATH = MODEL_DIR / \"best.pt\"\n",
        "INFERENCE_DIR = MODEL_DIR / \"inference_val\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Overview\n",
        "\n",
        "Compare dataset sizes before and after augmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we're using augmented dataset\n",
        "augmented_dataset_dir = BASE_DIR / \"datasets/ssd_mvtec_bottle_augmented\"\n",
        "original_dataset_dir = BASE_DIR / \"datasets/ssd_mvtec_bottle\"\n",
        "\n",
        "# Determine which dataset was used for training\n",
        "if DATASET_DIR.exists() and \"augmented\" in str(DATASET_DIR):\n",
        "    using_augmented = True\n",
        "    aug_info_path = DATASET_DIR / \"augmentation_info.json\"\n",
        "else:\n",
        "    using_augmented = False\n",
        "    # Check if augmented dataset exists even if not used\n",
        "    if augmented_dataset_dir.exists():\n",
        "        aug_info_path = augmented_dataset_dir / \"augmentation_info.json\"\n",
        "    else:\n",
        "        aug_info_path = None\n",
        "\n",
        "print(\"ðŸ“Š DATASET COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load original dataset info\n",
        "orig_info_path = original_dataset_dir / \"dataset_info.json\"\n",
        "if orig_info_path.exists():\n",
        "    with open(orig_info_path, 'r') as f:\n",
        "        orig_info = json.load(f)\n",
        "    \n",
        "    print(\"\\nâœ“ Original Dataset:\")\n",
        "    print(f\"   Train images: {orig_info.get('train_images', 0)}\")\n",
        "    print(f\"   Val images:   {orig_info.get('val_images', 0)}\")\n",
        "    print(f\"   Total:        {orig_info.get('train_images', 0) + orig_info.get('val_images', 0)}\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Original dataset info not found\")\n",
        "    orig_info = None\n",
        "\n",
        "# Load augmented dataset info\n",
        "if aug_info_path and aug_info_path.exists():\n",
        "    with open(aug_info_path, 'r') as f:\n",
        "        aug_info = json.load(f)\n",
        "    \n",
        "    stats = aug_info.get('stats', {})\n",
        "    train_stats = stats.get('train', {})\n",
        "    val_stats = stats.get('val', {})\n",
        "    \n",
        "    print(f\"\\nâœ“ Augmented Dataset (factor: {aug_info.get('augmentation_factor', 'N/A')}):\")\n",
        "    print(f\"   Train images: {train_stats.get('total', 0)} (original: {train_stats.get('original', 0)}, augmented: {train_stats.get('augmented', 0)})\")\n",
        "    print(f\"   Val images:   {val_stats.get('total', 0)} (original: {val_stats.get('original', 0)}, augmented: {val_stats.get('augmented', 0)})\")\n",
        "    print(f\"   Total:        {train_stats.get('total', 0) + val_stats.get('total', 0)}\")\n",
        "    \n",
        "    print(f\"\\nâœ“ Used for training: {'Augmented' if using_augmented else 'Original'} dataset\")\n",
        "    \n",
        "    # Create comparison DataFrame\n",
        "    comparison_data = []\n",
        "    \n",
        "    if orig_info:\n",
        "        comparison_data.append({\n",
        "            \"Dataset\": \"Original\",\n",
        "            \"Train\": orig_info.get('train_images', 0),\n",
        "            \"Val\": orig_info.get('val_images', 0),\n",
        "            \"Total\": orig_info.get('train_images', 0) + orig_info.get('val_images', 0)\n",
        "        })\n",
        "    \n",
        "    comparison_data.append({\n",
        "        \"Dataset\": \"Augmented\",\n",
        "        \"Train\": train_stats.get('total', 0),\n",
        "        \"Val\": val_stats.get('total', 0),\n",
        "        \"Total\": train_stats.get('total', 0) + val_stats.get('total', 0)\n",
        "    })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n\")\n",
        "    display(comparison_df)\n",
        "    \n",
        "    # Visualize comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Bar chart comparison\n",
        "    x = np.arange(len(comparison_data))\n",
        "    width = 0.25\n",
        "    \n",
        "    axes[0].bar(x - width, [d['Train'] for d in comparison_data], width, label='Train', color='#2E86AB')\n",
        "    axes[0].bar(x, [d['Val'] for d in comparison_data], width, label='Val', color='#A23B72')\n",
        "    axes[0].bar(x + width, [d['Total'] for d in comparison_data], width, label='Total', color='#F18F01')\n",
        "    \n",
        "    axes[0].set_ylabel('Number of Images')\n",
        "    axes[0].set_title('Dataset Comparison: Before and After Augmentation')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels([d['Dataset'] for d in comparison_data])\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Augmentation breakdown\n",
        "    if orig_info:\n",
        "        train_increase = ((train_stats.get('total', 0) - orig_info.get('train_images', 0)) / orig_info.get('train_images', 1)) * 100\n",
        "        val_increase = ((val_stats.get('total', 0) - orig_info.get('val_images', 0)) / orig_info.get('val_images', 1)) * 100\n",
        "        \n",
        "        axes[1].bar(['Train', 'Val'], [train_increase, val_increase], color=['#2E86AB', '#A23B72'])\n",
        "        axes[1].set_ylabel('Increase (%)')\n",
        "        axes[1].set_title('Dataset Growth After Augmentation')\n",
        "        axes[1].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        for i, (label, val) in enumerate(zip(['Train', 'Val'], [train_increase, val_increase])):\n",
        "            axes[1].text(i, val + 5, f'{val:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"\\nâš ï¸  Augmented dataset not found\")\n",
        "    print(\"Run augmentation first:\")\n",
        "    print(f\"python augment_ssd_dataset.py --dataset_dir {original_dataset_dir}\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Evaluation\n",
        "\n",
        "Analyze the model's performance on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load evaluation metrics\n",
        "metrics_path = INFERENCE_DIR / \"metrics.json\"\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "    \n",
        "    print(\"âœ“ Evaluation metrics loaded\\n\")\n",
        "    \n",
        "    # Display overall metrics\n",
        "    print(\"=\"*60)\n",
        "    print(\"OVERALL METRICS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Accuracy : {metrics.get('accuracy', 0)*100:>6.2f}%\")\n",
        "    print(f\"Precision: {metrics.get('precision', 0)*100:>6.2f}%\")\n",
        "    print(f\"Recall   : {metrics.get('recall', 0)*100:>6.2f}%\")\n",
        "    print(f\"F1-Score : {metrics.get('f1_score', 0)*100:>6.2f}%\")\n",
        "    print(f\"Defect detection rate: {metrics.get('defect_detection_rate', 0)*100:>6.2f}%\")\n",
        "    print(f\"Composite: {metrics.get('composite_score', 0)*100:>6.2f}%\")\n",
        "    print(f\"Confidence: {metrics.get('confidence', 0)*100:>6.2f}%\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Model Scorecard\n",
        "    scorecard_df = pd.DataFrame([\n",
        "        {\"Metric\": \"Accuracy\", \"Value\": f\"{metrics.get('accuracy', 0)*100:.2f}%\"},\n",
        "        {\"Metric\": \"Precision\", \"Value\": f\"{metrics.get('precision', 0)*100:.2f}%\"},\n",
        "        {\"Metric\": \"Recall\", \"Value\": f\"{metrics.get('recall', 0)*100:.2f}%\"},\n",
        "        {\"Metric\": \"F1-Score\", \"Value\": f\"{metrics.get('f1_score', 0)*100:.2f}%\"},\n",
        "        {\"Metric\": \"Defect detection rate\", \"Value\": f\"{metrics.get('defect_detection_rate', 0)*100:.2f}%\"},\n",
        "        {\"Metric\": \"Composite Score\", \"Value\": f\"{metrics.get('composite_score', 0)*100:.2f}%\"},\n",
        "        {\"Metric\": \"Confidence\", \"Value\": f\"{metrics.get('confidence', 0)*100:.2f}%\"},\n",
        "    ])\n",
        "    \n",
        "    print(\"\\nðŸ“Š Model Scorecard:\")\n",
        "    display(scorecard_df)\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    confusion_df = pd.DataFrame([\n",
        "        {\"Metric\": \"TP (Defect detected)\", \"Count\": metrics.get('tp_defect', 0)},\n",
        "        {\"Metric\": \"FP (False defect)\", \"Count\": metrics.get('fp_defect', 0)},\n",
        "        {\"Metric\": \"TN (Good correct)\", \"Count\": metrics.get('tn_good', 0)},\n",
        "        {\"Metric\": \"FN (Defect missed)\", \"Count\": metrics.get('fn_defect', 0)},\n",
        "        {\"Metric\": \"Total images\", \"Count\": metrics.get('total_images', 0)},\n",
        "    ])\n",
        "    \n",
        "    print(\"\\nðŸ“ˆ Confusion Matrix:\")\n",
        "    display(confusion_df)\n",
        "    \n",
        "else:\n",
        "    print(f\"âš ï¸ Metrics not found at {metrics_path}\")\n",
        "    print(\"Please run inference first:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sample Image Inspection\n",
        "\n",
        "Test the model on sample images (2 good + 2 defect) and visualize predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for SSD inference\n",
        "def load_ssd_model(model_path, num_classes=2, device='cpu'):\n",
        "    \"\"\"Load trained SSD model.\"\"\"\n",
        "    in_channels = [512, 1024, 512, 256, 256, 256]\n",
        "    num_anchors = [4, 6, 6, 6, 4, 4]\n",
        "    \n",
        "    model = ssd300_vgg16(num_classes=num_classes + 1)\n",
        "    model.head.classification_head = SSDClassificationHead(\n",
        "        in_channels=in_channels,\n",
        "        num_anchors=num_anchors,\n",
        "        num_classes=num_classes + 1\n",
        "    )\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def run_inference(model, image, device, conf_threshold=0.5):\n",
        "    \"\"\"Run inference on image array.\"\"\"\n",
        "    # Convert to RGB if needed\n",
        "    if len(image.shape) == 2:\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    elif image.shape[2] == 4:\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
        "    else:\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Convert to tensor\n",
        "    image_tensor = torch.from_numpy(image_rgb).permute(2, 0, 1).float() / 255.0\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "    \n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        predictions = model(image_tensor)\n",
        "    \n",
        "    # Extract predictions\n",
        "    pred = predictions[0]\n",
        "    boxes = pred['boxes'].cpu().numpy()\n",
        "    labels = pred['labels'].cpu().numpy()\n",
        "    scores = pred['scores'].cpu().numpy()\n",
        "    \n",
        "    # Filter by confidence threshold\n",
        "    mask = scores >= conf_threshold\n",
        "    boxes = boxes[mask]\n",
        "    labels = labels[mask]\n",
        "    scores = scores[mask]\n",
        "    \n",
        "    return boxes, labels, scores\n",
        "\n",
        "def draw_predictions(image, boxes, labels, scores, class_names=['background', 'good', 'defect']):\n",
        "    \"\"\"Draw bounding boxes on image.\"\"\"\n",
        "    image = image.copy()\n",
        "    \n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        \n",
        "        color = (0, 255, 0) if label == 1 else (0, 0, 255)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "        \n",
        "        class_name = class_names[label] if label < len(class_names) else f'class_{label}'\n",
        "        text = f\"{class_name}: {score:.2f}\"\n",
        "        cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def get_verdict(boxes, labels):\n",
        "    \"\"\"Get verdict: good or defect.\"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return \"GOOD\"\n",
        "    has_defect = 2 in labels\n",
        "    return \"DEFECT\" if has_defect else \"GOOD\"\n",
        "\n",
        "print(\"âœ“ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else \n",
        "                      'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model = load_ssd_model(MODEL_PATH, num_classes=2, device=device)\n",
        "print(\"âœ“ Model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select sample images (2 good + 2 defect)\n",
        "val_images_dir = DATASET_DIR / \"val\" / \"images\"\n",
        "val_ann_file = DATASET_DIR / \"val\" / \"annotations.json\"\n",
        "\n",
        "# Load annotations to find good and defect images\n",
        "with open(val_ann_file, 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "# Create mapping of image_id to annotations\n",
        "img_to_anns = {}\n",
        "for ann in val_data['annotations']:\n",
        "    img_id = ann['image_id']\n",
        "    if img_id not in img_to_anns:\n",
        "        img_to_anns[img_id] = []\n",
        "    img_to_anns[img_id].append(ann)\n",
        "\n",
        "# Find good and defect images\n",
        "good_images = []\n",
        "defect_images = []\n",
        "\n",
        "for img_info in val_data['images']:\n",
        "    img_id = img_info['id']\n",
        "    anns = img_to_anns.get(img_id, [])\n",
        "    \n",
        "    # Check if has defect (category_id == 2)\n",
        "    has_defect = any(ann['category_id'] == 2 for ann in anns)\n",
        "    \n",
        "    if has_defect:\n",
        "        defect_images.append(img_info)\n",
        "    else:\n",
        "        good_images.append(img_info)\n",
        "\n",
        "# Select 2 of each\n",
        "selected_defect = defect_images[:2] if len(defect_images) >= 2 else defect_images\n",
        "selected_good = good_images[:2] if len(good_images) >= 2 else good_images\n",
        "\n",
        "print(f\"Found {len(defect_images)} defect images, {len(good_images)} good images\")\n",
        "print(f\"Selected 2 defect + 2 good images for testing\")\n",
        "print(f\"\\nDefect images: {[img['file_name'] for img in selected_defect]}\")\n",
        "print(f\"Good images: {[img['file_name'] for img in selected_good]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize original images with predictions\n",
        "sample_images = selected_defect + selected_good\n",
        "sample_labels = ['Defect'] * len(selected_defect) + ['Good'] * len(selected_good)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "print(\"\\nRunning inference on sample images...\")\n",
        "\n",
        "for idx, (img_info, gt_label) in enumerate(zip(sample_images, sample_labels)):\n",
        "    img_path = val_images_dir / img_info['file_name']\n",
        "    \n",
        "    # Load image\n",
        "    image = cv2.imread(str(img_path))\n",
        "    \n",
        "    # Run inference\n",
        "    boxes, labels, scores = run_inference(model, image, device, conf_threshold=0.5)\n",
        "    \n",
        "    # Draw predictions\n",
        "    annotated_image = draw_predictions(image, boxes, labels, scores)\n",
        "    \n",
        "    # Get verdict\n",
        "    verdict = get_verdict(boxes, labels)\n",
        "    \n",
        "    # Display\n",
        "    axes[idx].imshow(annotated_image)\n",
        "    \n",
        "    # Create title\n",
        "    if len(boxes) > 0:\n",
        "        avg_conf = np.mean(scores)\n",
        "        title = f\"{img_info['file_name']}\\nGT: {gt_label} | Pred: {verdict} (conf: {avg_conf:.2f})\"\n",
        "        match = \"âœ“\" if gt_label.upper() == verdict else \"âœ—\"\n",
        "    else:\n",
        "        title = f\"{img_info['file_name']}\\nGT: {gt_label} | Pred: {verdict} (no detections)\"\n",
        "        match = \"âœ“\" if gt_label.upper() == verdict else \"âœ—\"\n",
        "    \n",
        "    title += f\" {match}\"\n",
        "    color = 'green' if match == \"âœ“\" else 'red'\n",
        "    \n",
        "    axes[idx].set_title(title, fontsize=10, color=color, fontweight='bold')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Original images displayed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Rotation Test\n",
        "\n",
        "Test model robustness by rotating images (0Â°, 90Â°, 180Â°, 270Â°) and checking if predictions remain consistent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test rotation on first image of each type\n",
        "test_images = [selected_defect[0], selected_good[0]]\n",
        "test_labels = ['Defect', 'Good']\n",
        "\n",
        "rotation_angles = {\n",
        "    '0Â°': None,\n",
        "    '90Â°': cv2.ROTATE_90_CLOCKWISE,\n",
        "    '180Â°': cv2.ROTATE_180,\n",
        "    '270Â°': cv2.ROTATE_90_COUNTERCLOCKWISE\n",
        "}\n",
        "\n",
        "print(\"Testing rotation robustness...\\n\")\n",
        "\n",
        "for img_info, gt_label in zip(test_images, test_labels):\n",
        "    img_path = val_images_dir / img_info['file_name']\n",
        "    original_image = cv2.imread(str(img_path))\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: {img_info['file_name']} (GT: {gt_label})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create subplot for this image\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    \n",
        "    results_summary = []\n",
        "    \n",
        "    for idx, (angle, rotation_code) in enumerate(rotation_angles.items()):\n",
        "        # Rotate image\n",
        "        if rotation_code is None:\n",
        "            rotated_image = original_image.copy()\n",
        "        else:\n",
        "            rotated_image = cv2.rotate(original_image, rotation_code)\n",
        "        \n",
        "        # Run inference\n",
        "        boxes, labels, scores = run_inference(model, rotated_image, device, conf_threshold=0.5)\n",
        "        \n",
        "        # Draw predictions\n",
        "        annotated_image = draw_predictions(rotated_image, boxes, labels, scores)\n",
        "        \n",
        "        # Get verdict\n",
        "        verdict = get_verdict(boxes, labels)\n",
        "        match = \"âœ“\" if gt_label.upper() == verdict else \"âœ—\"\n",
        "        \n",
        "        # Display\n",
        "        axes[idx].imshow(annotated_image)\n",
        "        \n",
        "        # Create title\n",
        "        if len(boxes) > 0:\n",
        "            avg_conf = np.mean(scores)\n",
        "            title = f\"Rotation: {angle}\\nPred: {verdict} (conf: {avg_conf:.2f}) {match}\"\n",
        "        else:\n",
        "            title = f\"Rotation: {angle}\\nPred: {verdict} (no detections) {match}\"\n",
        "        \n",
        "        color = 'green' if match == \"âœ“\" else 'red'\n",
        "        axes[idx].set_title(title, fontsize=9, color=color, fontweight='bold')\n",
        "        axes[idx].axis('off')\n",
        "        \n",
        "        # Store result\n",
        "        results_summary.append({\n",
        "            'Rotation': angle,\n",
        "            'Prediction': verdict,\n",
        "            'Detections': len(boxes),\n",
        "            'Avg Confidence': f\"{np.mean(scores):.2f}\" if len(scores) > 0 else \"N/A\",\n",
        "            'Match': match\n",
        "        })\n",
        "        \n",
        "        print(f\"  {angle:>4s}: Pred={verdict:>6s}, Detections={len(boxes)}, Match={match}\")\n",
        "    \n",
        "    plt.suptitle(f\"{img_info['file_name']} - GT: {gt_label}\", fontsize=12, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Display summary table\n",
        "    summary_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nRotation Test Summary:\")\n",
        "    display(summary_df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ“ Rotation test completed\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Complete Rotation Matrix\n",
        "\n",
        "Test all 4 sample images with all rotation angles in a comprehensive view.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive rotation matrix for all 4 images\n",
        "print(\"Creating complete rotation matrix...\\n\")\n",
        "\n",
        "# 4 images x 4 rotations = 16 subplots\n",
        "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "\n",
        "overall_results = []\n",
        "\n",
        "for row_idx, (img_info, gt_label) in enumerate(zip(sample_images, sample_labels)):\n",
        "    img_path = val_images_dir / img_info['file_name']\n",
        "    original_image = cv2.imread(str(img_path))\n",
        "    \n",
        "    for col_idx, (angle, rotation_code) in enumerate(rotation_angles.items()):\n",
        "        # Rotate image\n",
        "        if rotation_code is None:\n",
        "            rotated_image = original_image.copy()\n",
        "        else:\n",
        "            rotated_image = cv2.rotate(original_image, rotation_code)\n",
        "        \n",
        "        # Run inference\n",
        "        boxes, labels, scores = run_inference(model, rotated_image, device, conf_threshold=0.5)\n",
        "        \n",
        "        # Draw predictions\n",
        "        annotated_image = draw_predictions(rotated_image, boxes, labels, scores)\n",
        "        \n",
        "        # Get verdict\n",
        "        verdict = get_verdict(boxes, labels)\n",
        "        match = \"âœ“\" if gt_label.upper() == verdict else \"âœ—\"\n",
        "        \n",
        "        # Display\n",
        "        ax = axes[row_idx, col_idx]\n",
        "        ax.imshow(annotated_image)\n",
        "        \n",
        "        # Create title\n",
        "        if col_idx == 0:\n",
        "            # First column - show image name and GT\n",
        "            title = f\"{img_info['file_name'][:15]}...\\nGT: {gt_label} | {angle}\"\n",
        "        else:\n",
        "            title = angle\n",
        "        \n",
        "        if len(boxes) > 0:\n",
        "            avg_conf = np.mean(scores)\n",
        "            title += f\"\\n{verdict} ({avg_conf:.2f}) {match}\"\n",
        "        else:\n",
        "            title += f\"\\n{verdict} {match}\"\n",
        "        \n",
        "        color = 'green' if match == \"âœ“\" else 'red'\n",
        "        ax.set_title(title, fontsize=8, color=color, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Store result\n",
        "        overall_results.append({\n",
        "            'Image': img_info['file_name'],\n",
        "            'GT': gt_label,\n",
        "            'Rotation': angle,\n",
        "            'Prediction': verdict,\n",
        "            'Detections': len(boxes),\n",
        "            'Confidence': f\"{np.mean(scores):.2f}\" if len(scores) > 0 else \"0.00\",\n",
        "            'Correct': match\n",
        "        })\n",
        "\n",
        "plt.suptitle('Complete Rotation Test Matrix (2 Defect + 2 Good)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "results_df = pd.DataFrame(overall_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ROTATION TEST SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Overall accuracy\n",
        "correct_count = len(results_df[results_df['Correct'] == 'âœ“'])\n",
        "total_count = len(results_df)\n",
        "accuracy = (correct_count / total_count) * 100\n",
        "\n",
        "print(f\"\\nOverall Accuracy: {correct_count}/{total_count} ({accuracy:.1f}%)\")\n",
        "\n",
        "# By rotation angle\n",
        "print(\"\\nAccuracy by Rotation Angle:\")\n",
        "for angle in rotation_angles.keys():\n",
        "    angle_df = results_df[results_df['Rotation'] == angle]\n",
        "    correct = len(angle_df[angle_df['Correct'] == 'âœ“'])\n",
        "    total = len(angle_df)\n",
        "    acc = (correct / total) * 100 if total > 0 else 0\n",
        "    print(f\"  {angle:>4s}: {correct}/{total} ({acc:.1f}%)\")\n",
        "\n",
        "# By image type\n",
        "print(\"\\nAccuracy by Image Type:\")\n",
        "for label in ['Defect', 'Good']:\n",
        "    label_df = results_df[results_df['GT'] == label]\n",
        "    correct = len(label_df[label_df['Correct'] == 'âœ“'])\n",
        "    total = len(label_df)\n",
        "    acc = (correct / total) * 100 if total > 0 else 0\n",
        "    print(f\"  {label:>6s}: {correct}/{total} ({acc:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\nDetailed Results Table:\")\n",
        "display(results_df)\n",
        "\n",
        "print(\"\\nâœ“ Complete rotation matrix test finished\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
